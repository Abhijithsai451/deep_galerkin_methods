------------------------------------------------------------------------------------------------------------------------
Physics Informed Neural Networks - Implementation Plan
------------------------------------------------------------------------------------------------------------------------
1. Install the dependencies.
2. Neural Network Architecture:
    The Core of the PINN is the neural network, which acts as the solution u'(x,t,Ã¸)
    Define the Input: the network must accept the independent variables x and t (input size 2)
    Design the Model Class: We create a python class (PINN_Model) which inherits from torch.nn.Module.
        Architecture: We start with a simple MLP
            Layers: 4 - 6 Fully connected layers
            Neurons: 20 to 50 Neurons per layer.
            Activation: Typically tanh or SiLU (swish) for PINNs, as they perform better than ReLU for derivatives.
            Output Size: 1 (predicted temperature u)
3. Physics Loss Function:
    The Most crucial and unique part of PINNs. It enforces the PDE on the solution.
    Implement the Derivative Function : We create a helper function (or a method within your model class) to calculate the
    derivatives of the network output u with respect to the inputs x and t.
    We use pytorch function for Autograd
    Define the PDE Residual:
    Calculate the Physics Loss
4. Data Generation
    Boundary Condition Points
    Initial Condition Points
    Collocation Points
5. Total Loss Function
6. Training Loop
7. Evaluation and Visualization

------------------------------------------------------------------------------------------------------------------------
Differences Between DGM and PINNS
------------------------------------------------------------------------------------------------------------------------
Deep Galerkin Methods (DGM) and Deep Physics-Informed Neural Networks (PINNs) are both techniques that use neural
    networks to solve differential equations, but they differ in how they incorporate the governing equations and boundary
    conditions into the training process. Here's a detailed breakdown of their differences:
**Deep Galerkin Method (DGM)**
- **Residual Minimization:** DGM directly minimizes the residual of the differential equation. The neural network is
    trained to approximate the solution of the PDE, and the loss function is based on how well the neural network satisfies
    the equation at a set of randomly sampled points within the domain.
- **Loss Function:** The loss function typically consists of the squared residual of the PDE, evaluated at randomly
    sampled points. Boundary conditions are enforced by adding additional terms to the loss function that penalize
    deviations from the specified boundary values.
- **Collocation Points:** DGM relies on randomly sampled points (collocation points) within the domain to evaluate the
    PDE residual. The accuracy of the solution depends on the density and distribution of these points.
- **No Need for Derivatives in Loss:** DGM seeks to directly satisfy the equation, so the derivatives are calculated by
    autodifferentiation to calculate the loss.

**Deep Physics-Informed Neural Networks (PINNs)**
- **Physics-Informed Loss:** PINNs also minimize a loss function that includes the PDE residual. However, PINNs
    explicitly incorporate the physics of the problem into the neural network architecture and loss function.
- **Loss Function Components:** The loss function in PINNs typically consists of multiple components:
    - PDE Loss: Measures how well the neural network satisfies the governing differential equation.
    - Boundary Condition Loss: Enforces the specified boundary conditions.
    - Initial Condition Loss: Enforces the initial conditions (for time-dependent problems).
    - Data Loss (Optional): If available, incorporates experimental or observational data into the loss function.

- **Automatic Differentiation:** PINNs rely heavily on automatic differentiation to compute the derivatives of the
    neural network output with respect to the input coordinates. These derivatives are used to evaluate the PDE residual
    and enforce the physics of the problem.
- **Flexibility:** PINNs are more flexible in handling complex geometries, boundary conditions, and multi-physics problems.
    They can also be used to solve inverse problems, where the goal is to identify unknown parameters in the governing
    equations based on observed data.

**Key Differences Summarized**

| Feature | Deep Galerkin Method (DGM) | Deep Physics-Informed Neural Networks (PINNs) |
| --- | --- | --- |
| Loss Function | PDE Residual + Boundary Conditions | PDE Residual + Boundary/Initial Conditions + (Optional) Data Loss |
| Physics Integration | Implicit | Explicit |
| Derivative Calculation | Automatic Differentiation | Automatic Differentiation |
| Flexibility | Less | More |
**Similarities**
- Both DGM and PINNs use neural networks as function approximators.
- Both rely on automatic differentiation to compute derivatives.
- Both minimize a loss function to train the neural network.

**In Essence**
DGM can be seen as a specific implementation of the PINN approach. PINNs are a more general framework that allows for
    greater flexibility in incorporating various types of physics-related information into the training process.
    The main distinction lies in how explicitly the physics are embedded within the loss function and, potentially,
    the network architecture.


------------------------------------------------------------------------------------------------------------------------
References
------------------------------------------------------------------------------------------------------------------------

Network Architecture: (DGM) -> https://github.com/alialaradi/DeepGalerkinMethod
